## Seeding and Sorting Overview

Seeders and Sorters canvass the resources of a given government agency, identifying important URLs. They identify whether those URLs can be crawled by the Internet Archive's webcrawler. If the URLs are crawlable, the Seeders/Sorters nominate them to the End-of-Term (EOT) project, otherwise they add them to the Uncrawlable spreadsheet using the project's Chrome Extension.

# Choosing the website
The Seeders/Sorters team will use the EDGI subprimer systems ([found here](https://envirodatagov.org/agency-forecasts/)), or a similar set of resources, to identify important/at risk data. Talk to the DataRescue organizers to learn more.

# Canvassing the website
- Canvass the resources of a given government agency, identifying important URLs. 
They sort them by whether their data can be automatically captured by the Internet Archive webcrawler (about which more here and here).
 
URLs judged to be possibly crawlable are "nominated" (equivalently, "seeded") using our Chrome extension or bookmarklet. This sorting is only provisional: when in doubt seeders mark a URL as possibly not crawlable, and these URLs populate a spreadsheet.
The best source of information about the seeding and sorting process is represented at [https://envirodatagov.org/](https://envirodatagov.org/).

 Individual events should set up spreadsheets or other tools in which search efforts can be recorded. The work of this group includes:

- Canvassing the resources of a given government agency, identifying important URLs.
- Identifying whether those URL's [can be crawled by the Internet Archive's webcrawler](./what-heritrix-does.md)
    - If URL's are crawlable, nominate them to the EOT crawl using the [EDGI Nomination Tool](https://chrome.google.com/webstore/detail/nominationtool/abjpihafglmijnkkoppbookfkkanklok?hl=en)
    - If they are not crawlable, add them to the "Uncrawlable" spreadsheet through the Chrome Extension. 



- The *Seeders and Sorters* team canvases the resources of a given government
  agency, identifying important URLs. They sort them by whether their data
  can be automatically captured by the Internet Archive webcrawler (about which
  more
  [here](https://docs.google.com/document/d/1PeWefW2toThs-Pbw0CMv2us7wxQI0gRrP1LGuwMp_UQ/edit)
  and
  [here](https://docs.google.com/document/d/1qpuNCmBmu4KcsS_hE2srewcCiP4f9P5cCyDfHmsSAVU/edit)).
  URLs judged to be possibly crawlable are "nominated" (equivalently, "seeded")
  using our
  [Chrome extension](https://chrome.google.com/webstore/detail/nominationtool/abjpihafglmijnkkoppbookfkkanklok)
  or
  [bookmarklet](http://digital2.library.unt.edu/nomination/eth2016/about/).
  This sorting is only provisional: when in doubt seeders mark a URL as possibly
  *not* crawlable, and these URLs populate a spreadsheet.
  
- If a URL does not seem crawlable:
  - It is added to the Unscrawlable spreadsheet through the Chrome extention.
  - It is automatically associated with a universal unique identifyer (UUID) that was generated in advance. 
